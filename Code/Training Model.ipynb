{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Generator Model Training\n",
    "\n",
    "In this notebook, I aim to train a model to generate TV scripts using Deep Learning. For training purposes, I have decided to use the scripts from all 10 seasons of F.R.I.E.N.D.S as it is one of my favorite shows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import os\n",
    "import util\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import problem_unittests as tests\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Check for GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Data\n",
    "In this step, I load the data from a .txt file. \n",
    "I then explore it a little to get a sense of what I'm working with. \n",
    "\n",
    "You can skip to the checkpoint without exploring the text files if you wish to train the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Data/friends.txt'\n",
    "text = util.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 54785\n",
      "Number of lines: 104959\n",
      "Average number of words in each line: 8.59682352157\n",
      "()\n",
      "The lines 303 to 320:\n",
      "Paul: No, it's, it's more of a fifth date kinda\n",
      "revelation.\n",
      "Monica: Oh, so there is gonna be a fifth date?\n",
      "Paul: Isn't there?\n",
      "Monica: Yeah... yeah, I think there is. -What were\n",
      "you gonna say?\n",
      "Paul: Well, ever-ev-... ever since she left me, um,\n",
      "I haven't been able to, uh, perform. (Monica takes a sip of her drink.) ...Sexually.\n",
      "Monica: (spitting out her drink in shock) Oh God, oh\n",
      "God, I am sorry... I am so sorry...\n",
      "Paul: It's okay...\n",
      "Monica: I know being spit on is probably not what\n",
      "you need right now. Um... how long?\n",
      "Paul: Two years.\n",
      "Monica: Wow! I'm-I'm-I'm glad you smashed her watch!\n",
      "Paul: So you still think you, um... might want that\n",
      "fifth date?\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (303, 320) #prints text between the given lines\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data\n",
    "In this step, I pre-process the input data so that the model can train on it. \n",
    "1. I create a lookup table that maps words to indexes\n",
    "2. I create a token dictionary to seperate punctuations from regular words.\n",
    "\n",
    "I save these dictionaries so that we don't have to pre-process it each time we run the notebook. \n",
    "You can directly skip to Checkpoint without running the next three blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_idx, idx_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab = set(text)\n",
    "    \n",
    "    # Use comprenhension lists to build our dictionaries.\n",
    "    vocab_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_vocab = {idx:word for idx, word in enumerate(vocab)}\n",
    "    return (vocab_to_idx, idx_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    return {'.':'||Period||',\n",
    "            ',':'||Comma||',\n",
    "            '\"':'||Quotation_Mark||',\n",
    "            ';':'||Semicolon||', \n",
    "            '!':'||Exclamation_Mark||',\n",
    "            '?':'||Question_Mark||', \n",
    "            '(':'||Left_Parentheses||', \n",
    "            ')':'||Right_Parentheses||',\n",
    "            '-':'||Dash||',\n",
    "            '\\n':'||Return||'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data and Save it\n",
    "util.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: \n",
    "### 1. Load the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_idx, idx_to_vocab, token_dict = util.load_preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086209\n"
     ]
    }
   ],
   "source": [
    "print(len(int_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    n_batches = len(words)//batch_size\n",
    "    words = words[:n_batches*batch_size]\n",
    "    y_len = len(words) - sequence_length\n",
    "    x, y = [], []\n",
    "    for idx in range(0, y_len):\n",
    "        idx_end = sequence_length + idx\n",
    "        x_batch = words[idx:idx_end]\n",
    "        x.append(x_batch)\n",
    "        batch_y =  words[idx_end]  \n",
    "        y.append(batch_y)    \n",
    "    \n",
    "    print(len(x), len(y))\n",
    "    #Create Tensor datasets\n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    data_loader = DataLoader(data, shuffle=False, batch_size=batch_size)\n",
    "  \n",
    "    return data_loader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5,lr=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "               \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "    \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(0),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(0))\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda(0)\n",
    "\n",
    "    h = tuple([each.data for each in hidden])\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inputs, target = inp.cuda(0), target.cuda(0)\n",
    "\n",
    "    output, h = rnn(inputs, h)\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_value_(rnn.parameters(), 1) #clip_grad_value_ - 1\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Model\n",
    "\n",
    "You can re-train the model by running the next four blocks. \n",
    "Note: Training takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1086198, 1086198)\n"
     ]
    }
   ],
   "source": [
    "#Data parameters\n",
    "sequence_length = 10 \n",
    "batch_size = 128 \n",
    "\n",
    "#Load batched data\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "num_epochs = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Model parameters\n",
    "vocab_size = len(vocab_to_idx)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "#Show stats for every n number of batches\n",
    "show_every_n_batches = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    losses = []\n",
    "    rnn.train()\n",
    "    #Make sure you iterate over completely full batches, only\n",
    "    n_batches = len(train_loader.dataset)//batch_size\n",
    "    \n",
    "    #Initialize hidden state\n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            #Forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            #Record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            #Printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                avg = np.average(batch_losses)\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, avg))\n",
    "                losses.append([batch_i, avg])\n",
    "                batch_losses = []\n",
    "\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 128 epoch(s)...\n",
      "Epoch:    1/128   Loss: 5.22909194422\n",
      "\n",
      "Epoch:    1/128   Loss: 4.74667523277\n",
      "\n",
      "Epoch:    1/128   Loss: 4.69687299383\n",
      "\n",
      "Epoch:    1/128   Loss: 4.70357798135\n",
      "\n",
      "Epoch:    2/128   Loss: 4.44755326977\n",
      "\n",
      "Epoch:    2/128   Loss: 4.25997623396\n",
      "\n",
      "Epoch:    2/128   Loss: 4.30073728991\n",
      "\n",
      "Epoch:    2/128   Loss: 4.29542011225\n",
      "\n",
      "Epoch:    3/128   Loss: 4.1677004982\n",
      "\n",
      "Epoch:    3/128   Loss: 4.04923562765\n",
      "\n",
      "Epoch:    3/128   Loss: 4.10956633604\n",
      "\n",
      "Epoch:    3/128   Loss: 4.09766844451\n",
      "\n",
      "Epoch:    4/128   Loss: 3.98008153894\n",
      "\n",
      "Epoch:    4/128   Loss: 3.90513658226\n",
      "\n",
      "Epoch:    4/128   Loss: 3.96116559172\n",
      "\n",
      "Epoch:    4/128   Loss: 3.94743908989\n",
      "\n",
      "Epoch:    5/128   Loss: 3.84947528925\n",
      "\n",
      "Epoch:    5/128   Loss: 3.79161346495\n",
      "\n",
      "Epoch:    5/128   Loss: 3.8454236778\n",
      "\n",
      "Epoch:    5/128   Loss: 3.83603486192\n",
      "\n",
      "Epoch:    6/128   Loss: 3.7486120957\n",
      "\n",
      "Epoch:    6/128   Loss: 3.70206538689\n",
      "\n",
      "Epoch:    6/128   Loss: 3.7610859431\n",
      "\n",
      "Epoch:    6/128   Loss: 3.74780666864\n",
      "\n",
      "Epoch:    7/128   Loss: 3.66739134913\n",
      "\n",
      "Epoch:    7/128   Loss: 3.62835680664\n",
      "\n",
      "Epoch:    7/128   Loss: 3.68674888539\n",
      "\n",
      "Epoch:    7/128   Loss: 3.67922172987\n",
      "\n",
      "Epoch:    8/128   Loss: 3.6016671648\n",
      "\n",
      "Epoch:    8/128   Loss: 3.56481925011\n",
      "\n",
      "Epoch:    8/128   Loss: 3.62949724472\n",
      "\n",
      "Epoch:    8/128   Loss: 3.61501815736\n",
      "\n",
      "Epoch:    9/128   Loss: 3.5436871639\n",
      "\n",
      "Epoch:    9/128   Loss: 3.51664669573\n",
      "\n",
      "Epoch:    9/128   Loss: 3.57751339829\n",
      "\n",
      "Epoch:    9/128   Loss: 3.56082327652\n",
      "\n",
      "Epoch:   10/128   Loss: 3.49458194316\n",
      "\n",
      "Epoch:   10/128   Loss: 3.4704453336\n",
      "\n",
      "Epoch:   10/128   Loss: 3.53231083715\n",
      "\n",
      "Epoch:   10/128   Loss: 3.51678036976\n",
      "\n",
      "Epoch:   11/128   Loss: 3.45021624392\n",
      "\n",
      "Epoch:   11/128   Loss: 3.43074121857\n",
      "\n",
      "Epoch:   11/128   Loss: 3.49264979172\n",
      "\n",
      "Epoch:   11/128   Loss: 3.47793136191\n",
      "\n",
      "Epoch:   12/128   Loss: 3.41538311999\n",
      "\n",
      "Epoch:   12/128   Loss: 3.39536033082\n",
      "\n",
      "Epoch:   12/128   Loss: 3.45907793391\n",
      "\n",
      "Epoch:   12/128   Loss: 3.44093413031\n",
      "\n",
      "Epoch:   13/128   Loss: 3.37922573368\n",
      "\n",
      "Epoch:   13/128   Loss: 3.36426308072\n",
      "\n",
      "Epoch:   13/128   Loss: 3.42880861616\n",
      "\n",
      "Epoch:   13/128   Loss: 3.41340626109\n",
      "\n",
      "Epoch:   14/128   Loss: 3.34547612451\n",
      "\n",
      "Epoch:   14/128   Loss: 3.33505630505\n",
      "\n",
      "Epoch:   14/128   Loss: 3.4005959357\n",
      "\n",
      "Epoch:   14/128   Loss: 3.38458281708\n",
      "\n",
      "Epoch:   15/128   Loss: 3.31983873964\n",
      "\n",
      "Epoch:   15/128   Loss: 3.31003698945\n",
      "\n",
      "Epoch:   15/128   Loss: 3.37443198431\n",
      "\n",
      "Epoch:   15/128   Loss: 3.35369934905\n",
      "\n",
      "Epoch:   16/128   Loss: 3.29368194776\n",
      "\n",
      "Epoch:   16/128   Loss: 3.28370209122\n",
      "\n",
      "Epoch:   16/128   Loss: 3.35306620634\n",
      "\n",
      "Epoch:   16/128   Loss: 3.32912106705\n",
      "\n",
      "Epoch:   17/128   Loss: 3.27014213251\n",
      "\n",
      "Epoch:   17/128   Loss: 3.25922105229\n",
      "\n",
      "Epoch:   17/128   Loss: 3.33261126065\n",
      "\n",
      "Epoch:   17/128   Loss: 3.30561093879\n",
      "\n",
      "Epoch:   18/128   Loss: 3.24470392079\n",
      "\n",
      "Epoch:   18/128   Loss: 3.23899361897\n",
      "\n",
      "Epoch:   18/128   Loss: 3.31264924109\n",
      "\n",
      "Epoch:   18/128   Loss: 3.28621250176\n",
      "\n",
      "Epoch:   19/128   Loss: 3.22275194599\n",
      "\n",
      "Epoch:   19/128   Loss: 3.22355830693\n",
      "\n",
      "Epoch:   19/128   Loss: 3.2910479793\n",
      "\n",
      "Epoch:   19/128   Loss: 3.26560399413\n",
      "\n",
      "Epoch:   20/128   Loss: 3.20409656972\n",
      "\n",
      "Epoch:   20/128   Loss: 3.20359710693\n",
      "\n",
      "Epoch:   20/128   Loss: 3.27320858109\n",
      "\n",
      "Epoch:   20/128   Loss: 3.24870715189\n",
      "\n",
      "Epoch:   21/128   Loss: 3.18500565504\n",
      "\n",
      "Epoch:   21/128   Loss: 3.18794235158\n",
      "\n",
      "Epoch:   21/128   Loss: 3.25911901778\n",
      "\n",
      "Epoch:   21/128   Loss: 3.22922660732\n",
      "\n",
      "Epoch:   22/128   Loss: 3.17043139791\n",
      "\n",
      "Epoch:   22/128   Loss: 3.17356759989\n",
      "\n",
      "Epoch:   22/128   Loss: 3.2421418066\n",
      "\n",
      "Epoch:   22/128   Loss: 3.21106712687\n",
      "\n",
      "Epoch:   23/128   Loss: 3.1535868071\n",
      "\n",
      "Epoch:   23/128   Loss: 3.16011182731\n",
      "\n",
      "Epoch:   23/128   Loss: 3.22971153128\n",
      "\n",
      "Epoch:   23/128   Loss: 3.20116804278\n",
      "\n",
      "Epoch:   24/128   Loss: 3.13849650173\n",
      "\n",
      "Epoch:   24/128   Loss: 3.14844825411\n",
      "\n",
      "Epoch:   24/128   Loss: 3.21696392554\n",
      "\n",
      "Epoch:   24/128   Loss: 3.17865166748\n",
      "\n",
      "Epoch:   25/128   Loss: 3.12277648065\n",
      "\n",
      "Epoch:   25/128   Loss: 3.13694606805\n",
      "\n",
      "Epoch:   25/128   Loss: 3.20362211168\n",
      "\n",
      "Epoch:   25/128   Loss: 3.16654532015\n",
      "\n",
      "Epoch:   26/128   Loss: 3.10612345073\n",
      "\n",
      "Epoch:   26/128   Loss: 3.12342224067\n",
      "\n",
      "Epoch:   26/128   Loss: 3.19135060436\n",
      "\n",
      "Epoch:   26/128   Loss: 3.15081008089\n",
      "\n",
      "Epoch:   27/128   Loss: 3.09761479299\n",
      "\n",
      "Epoch:   27/128   Loss: 3.11324857903\n",
      "\n",
      "Epoch:   27/128   Loss: 3.18092596501\n",
      "\n",
      "Epoch:   27/128   Loss: 3.14017410219\n",
      "\n",
      "Epoch:   28/128   Loss: 3.0830186392\n",
      "\n",
      "Epoch:   28/128   Loss: 3.10153787845\n",
      "\n",
      "Epoch:   28/128   Loss: 3.16956446272\n",
      "\n",
      "Epoch:   28/128   Loss: 3.13078672564\n",
      "\n",
      "Epoch:   29/128   Loss: 3.07510769041\n",
      "\n",
      "Epoch:   29/128   Loss: 3.08872322178\n",
      "\n",
      "Epoch:   29/128   Loss: 3.15737805599\n",
      "\n",
      "Epoch:   29/128   Loss: 3.11935156047\n",
      "\n",
      "Epoch:   30/128   Loss: 3.06068969919\n",
      "\n",
      "Epoch:   30/128   Loss: 3.07974171692\n",
      "\n",
      "Epoch:   30/128   Loss: 3.15093703353\n",
      "\n",
      "Epoch:   30/128   Loss: 3.10901246607\n",
      "\n",
      "Epoch:   31/128   Loss: 3.05152949778\n",
      "\n",
      "Epoch:   31/128   Loss: 3.07828254205\n",
      "\n",
      "Epoch:   31/128   Loss: 3.1428619529\n",
      "\n",
      "Epoch:   31/128   Loss: 3.09819627273\n",
      "\n",
      "Epoch:   32/128   Loss: 3.04551937863\n",
      "\n",
      "Epoch:   32/128   Loss: 3.0671967178\n",
      "\n",
      "Epoch:   32/128   Loss: 3.13246390563\n",
      "\n",
      "Epoch:   32/128   Loss: 3.08865545797\n",
      "\n",
      "Epoch:   33/128   Loss: 3.03499095944\n",
      "\n",
      "Epoch:   33/128   Loss: 3.05707774091\n",
      "\n",
      "Epoch:   33/128   Loss: 3.12210849386\n",
      "\n",
      "Epoch:   33/128   Loss: 3.08018552715\n",
      "\n",
      "Epoch:   34/128   Loss: 3.02317135819\n",
      "\n",
      "Epoch:   34/128   Loss: 3.04882230997\n",
      "\n",
      "Epoch:   34/128   Loss: 3.11183186662\n",
      "\n",
      "Epoch:   34/128   Loss: 3.07488993347\n",
      "\n",
      "Epoch:   35/128   Loss: 3.01633478641\n",
      "\n",
      "Epoch:   35/128   Loss: 3.04242291152\n",
      "\n",
      "Epoch:   35/128   Loss: 3.1084590323\n",
      "\n",
      "Epoch:   35/128   Loss: 3.06452898216\n",
      "\n",
      "Epoch:   36/128   Loss: 3.00638967592\n",
      "\n",
      "Epoch:   36/128   Loss: 3.03243522233\n",
      "\n",
      "Epoch:   36/128   Loss: 3.10234974831\n",
      "\n",
      "Epoch:   36/128   Loss: 3.06001859224\n",
      "\n",
      "Epoch:   37/128   Loss: 3.0033369569\n",
      "\n",
      "Epoch:   37/128   Loss: 3.02785723317\n",
      "\n",
      "Epoch:   37/128   Loss: 3.09720802152\n",
      "\n",
      "Epoch:   37/128   Loss: 3.04837628722\n",
      "\n",
      "Epoch:   38/128   Loss: 2.9950188395\n",
      "\n",
      "Epoch:   38/128   Loss: 3.02047918826\n",
      "\n",
      "Epoch:   38/128   Loss: 3.08561029917\n",
      "\n",
      "Epoch:   38/128   Loss: 3.04296263146\n",
      "\n",
      "Epoch:   39/128   Loss: 2.9895679471\n",
      "\n",
      "Epoch:   39/128   Loss: 3.01225078917\n",
      "\n",
      "Epoch:   39/128   Loss: 3.07926480889\n",
      "\n",
      "Epoch:   39/128   Loss: 3.03360514069\n",
      "\n",
      "Epoch:   40/128   Loss: 2.97885167743\n",
      "\n",
      "Epoch:   40/128   Loss: 3.00123095953\n",
      "\n",
      "Epoch:   40/128   Loss: 3.07747271687\n",
      "\n",
      "Epoch:   40/128   Loss: 3.02661442858\n",
      "\n",
      "Epoch:   41/128   Loss: 2.97301823345\n",
      "\n",
      "Epoch:   41/128   Loss: 3.00002582628\n",
      "\n",
      "Epoch:   41/128   Loss: 3.06436014372\n",
      "\n",
      "Epoch:   41/128   Loss: 3.02025769883\n",
      "\n",
      "Epoch:   42/128   Loss: 2.96576443995\n",
      "\n",
      "Epoch:   42/128   Loss: 2.99800611377\n",
      "\n",
      "Epoch:   42/128   Loss: 3.05928090501\n",
      "\n",
      "Epoch:   42/128   Loss: 3.01512429082\n",
      "\n",
      "Epoch:   43/128   Loss: 2.95982253312\n",
      "\n",
      "Epoch:   43/128   Loss: 2.99412635863\n",
      "\n",
      "Epoch:   43/128   Loss: 3.0560067718\n",
      "\n",
      "Epoch:   43/128   Loss: 3.00686857402\n",
      "\n",
      "Epoch:   44/128   Loss: 2.95511567194\n",
      "\n",
      "Epoch:   44/128   Loss: 2.98692415941\n",
      "\n",
      "Epoch:   44/128   Loss: 3.04964869231\n",
      "\n",
      "Epoch:   44/128   Loss: 2.99918303108\n",
      "\n",
      "Epoch:   45/128   Loss: 2.95239284087\n",
      "\n",
      "Epoch:   45/128   Loss: 2.98110137904\n",
      "\n",
      "Epoch:   45/128   Loss: 3.04819224191\n",
      "\n",
      "Epoch:   45/128   Loss: 2.99675988591\n",
      "\n",
      "Epoch:   46/128   Loss: 2.94150755132\n",
      "\n",
      "Epoch:   46/128   Loss: 2.97676856571\n",
      "\n",
      "Epoch:   46/128   Loss: 3.03977869594\n",
      "\n",
      "Epoch:   46/128   Loss: 2.99136856723\n",
      "\n",
      "Epoch:   47/128   Loss: 2.94103364129\n",
      "\n",
      "Epoch:   47/128   Loss: 2.97011281765\n",
      "\n",
      "Epoch:   47/128   Loss: 3.03392635095\n",
      "\n",
      "Epoch:   47/128   Loss: 2.98561846387\n",
      "\n",
      "Epoch:   48/128   Loss: 2.96799014354\n",
      "\n",
      "Epoch:   48/128   Loss: 3.02773481268\n",
      "\n",
      "Epoch:   48/128   Loss: 2.97889151049\n",
      "\n",
      "Epoch:   49/128   Loss: 2.92454373357\n",
      "\n",
      "Epoch:   49/128   Loss: 2.96635686231\n",
      "\n",
      "Epoch:   49/128   Loss: 3.02330429918\n",
      "\n",
      "Epoch:   49/128   Loss: 2.972529181\n",
      "\n",
      "Epoch:   50/128   Loss: 2.92358914731\n",
      "\n",
      "Epoch:   50/128   Loss: 2.95693446887\n",
      "\n",
      "Epoch:   50/128   Loss: 3.0186652292\n",
      "\n",
      "Epoch:   50/128   Loss: 2.97045594436\n",
      "\n",
      "Epoch:   51/128   Loss: 2.91934147879\n",
      "\n",
      "Epoch:   51/128   Loss: 2.9527638697\n",
      "\n",
      "Epoch:   51/128   Loss: 3.01005102152\n",
      "\n",
      "Epoch:   51/128   Loss: 2.96285482824\n",
      "\n",
      "Epoch:   52/128   Loss: 2.91635674135\n",
      "\n",
      "Epoch:   52/128   Loss: 2.94801288384\n",
      "\n",
      "Epoch:   52/128   Loss: 3.00902506351\n",
      "\n",
      "Epoch:   52/128   Loss: 2.95760070664\n",
      "\n",
      "Epoch:   53/128   Loss: 2.91055052261\n",
      "\n",
      "Epoch:   53/128   Loss: 2.9460266332\n",
      "\n",
      "Epoch:   53/128   Loss: 3.00506659549\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   53/128   Loss: 2.95581162918\n",
      "\n",
      "Epoch:   54/128   Loss: 2.90120667442\n",
      "\n",
      "Epoch:   54/128   Loss: 2.94101360506\n",
      "\n",
      "Epoch:   54/128   Loss: 3.0039335767\n",
      "\n",
      "Epoch:   54/128   Loss: 2.94973499125\n",
      "\n",
      "Epoch:   55/128   Loss: 2.89741489571\n",
      "\n",
      "Epoch:   55/128   Loss: 2.94192066336\n",
      "\n",
      "Epoch:   55/128   Loss: 2.9995897705\n",
      "\n",
      "Epoch:   55/128   Loss: 2.95000543618\n",
      "\n",
      "Epoch:   56/128   Loss: 2.8970115014\n",
      "\n",
      "Epoch:   56/128   Loss: 2.93874767721\n",
      "\n",
      "Epoch:   56/128   Loss: 2.99495403409\n",
      "\n",
      "Epoch:   56/128   Loss: 2.94015707648\n",
      "\n",
      "Epoch:   57/128   Loss: 2.89037267999\n",
      "\n",
      "Epoch:   57/128   Loss: 2.93645435935\n",
      "\n",
      "Epoch:   57/128   Loss: 2.99185612679\n",
      "\n",
      "Epoch:   57/128   Loss: 2.93587187797\n",
      "\n",
      "Epoch:   58/128   Loss: 2.88881511026\n",
      "\n",
      "Epoch:   58/128   Loss: 2.93039199924\n",
      "\n",
      "Epoch:   58/128   Loss: 2.98743226004\n",
      "\n",
      "Epoch:   58/128   Loss: 2.93263111281\n",
      "\n",
      "Epoch:   59/128   Loss: 2.88556299507\n",
      "\n",
      "Epoch:   59/128   Loss: 2.9311093033\n",
      "\n",
      "Epoch:   59/128   Loss: 2.98363184774\n",
      "\n",
      "Epoch:   59/128   Loss: 2.93331667107\n",
      "\n",
      "Epoch:   60/128   Loss: 2.87937271518\n",
      "\n",
      "Epoch:   60/128   Loss: 2.92380788058\n",
      "\n",
      "Epoch:   60/128   Loss: 2.98125319064\n",
      "\n",
      "Epoch:   60/128   Loss: 2.92576141423\n",
      "\n",
      "Epoch:   61/128   Loss: 2.87556868381\n",
      "\n",
      "Epoch:   61/128   Loss: 2.9192432856\n",
      "\n",
      "Epoch:   61/128   Loss: 2.97741700613\n",
      "\n",
      "Epoch:   61/128   Loss: 2.92836941606\n",
      "\n",
      "Epoch:   62/128   Loss: 2.87543045256\n",
      "\n",
      "Epoch:   62/128   Loss: 2.91754204983\n",
      "\n",
      "Epoch:   62/128   Loss: 2.97760254729\n",
      "\n",
      "Epoch:   62/128   Loss: 2.92141355151\n",
      "\n",
      "Epoch:   63/128   Loss: 2.86801523011\n",
      "\n",
      "Epoch:   63/128   Loss: 2.91418883169\n",
      "\n",
      "Epoch:   63/128   Loss: 2.9679670254\n",
      "\n",
      "Epoch:   63/128   Loss: 2.91660517424\n",
      "\n",
      "Epoch:   64/128   Loss: 2.86721781298\n",
      "\n",
      "Epoch:   64/128   Loss: 2.91642979819\n",
      "\n",
      "Epoch:   64/128   Loss: 2.96887535757\n",
      "\n",
      "Epoch:   64/128   Loss: 2.90834376162\n",
      "\n",
      "Epoch:   65/128   Loss: 2.86603949828\n",
      "\n",
      "Epoch:   65/128   Loss: 2.9121819399\n",
      "\n",
      "Epoch:   65/128   Loss: 2.96507381767\n",
      "\n",
      "Epoch:   65/128   Loss: 2.90909340632\n",
      "\n",
      "Epoch:   66/128   Loss: 2.85860701869\n",
      "\n",
      "Epoch:   66/128   Loss: 2.90637140369\n",
      "\n",
      "Epoch:   66/128   Loss: 2.96216260892\n",
      "\n",
      "Epoch:   66/128   Loss: 2.90332367426\n",
      "\n",
      "Epoch:   67/128   Loss: 2.85731613866\n",
      "\n",
      "Epoch:   67/128   Loss: 2.90582752275\n",
      "\n",
      "Epoch:   67/128   Loss: 2.95758705962\n",
      "\n",
      "Epoch:   67/128   Loss: 2.91058164579\n",
      "\n",
      "Epoch:   68/128   Loss: 2.85315085777\n",
      "\n",
      "Epoch:   68/128   Loss: 2.90153659809\n",
      "\n",
      "Epoch:   68/128   Loss: 2.95472055876\n",
      "\n",
      "Epoch:   68/128   Loss: 2.89886078727\n",
      "\n",
      "Epoch:   69/128   Loss: 2.84724175014\n",
      "\n",
      "Epoch:   69/128   Loss: 2.8953680985\n",
      "\n",
      "Epoch:   69/128   Loss: 2.9547506671\n",
      "\n",
      "Epoch:   69/128   Loss: 2.90025142545\n",
      "\n",
      "Epoch:   70/128   Loss: 2.8518704424\n",
      "\n",
      "Epoch:   70/128   Loss: 2.89223355341\n",
      "\n",
      "Epoch:   70/128   Loss: 2.95244090676\n",
      "\n",
      "Epoch:   70/128   Loss: 2.89724381334\n",
      "\n",
      "Epoch:   71/128   Loss: 2.84349785743\n",
      "\n",
      "Epoch:   71/128   Loss: 2.89370307469\n",
      "\n",
      "Epoch:   71/128   Loss: 2.94888255841\n",
      "\n",
      "Epoch:   71/128   Loss: 2.89331208175\n",
      "\n",
      "Epoch:   72/128   Loss: 2.8410202496\n",
      "\n",
      "Epoch:   72/128   Loss: 2.89021544582\n",
      "\n",
      "Epoch:   72/128   Loss: 2.94552510071\n",
      "\n",
      "Epoch:   72/128   Loss: 2.88473639649\n",
      "\n",
      "Epoch:   73/128   Loss: 2.84049703974\n",
      "\n",
      "Epoch:   73/128   Loss: 2.88913372946\n",
      "\n",
      "Epoch:   73/128   Loss: 2.94421782637\n",
      "\n",
      "Epoch:   73/128   Loss: 2.88805077553\n",
      "\n",
      "Epoch:   74/128   Loss: 2.83324688906\n",
      "\n",
      "Epoch:   74/128   Loss: 2.88448068726\n",
      "\n",
      "Epoch:   74/128   Loss: 2.94075392491\n",
      "\n",
      "Epoch:   74/128   Loss: 2.88853970474\n",
      "\n",
      "Epoch:   75/128   Loss: 2.83858804674\n",
      "\n",
      "Epoch:   75/128   Loss: 2.87790066391\n",
      "\n",
      "Epoch:   75/128   Loss: 2.94181979716\n",
      "\n",
      "Epoch:   75/128   Loss: 2.88402152133\n",
      "\n",
      "Epoch:   76/128   Loss: 2.8275831035\n",
      "\n",
      "Epoch:   76/128   Loss: 2.88291169167\n",
      "\n",
      "Epoch:   76/128   Loss: 2.93491226113\n",
      "\n",
      "Epoch:   76/128   Loss: 2.88530832243\n",
      "\n",
      "Epoch:   77/128   Loss: 2.82601459962\n",
      "\n",
      "Epoch:   77/128   Loss: 2.87974886757\n",
      "\n",
      "Epoch:   77/128   Loss: 2.93273958182\n",
      "\n",
      "Epoch:   77/128   Loss: 2.88015734679\n",
      "\n",
      "Epoch:   78/128   Loss: 2.82245796821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "#Create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "\n",
    "if(train_on_gpu):\n",
    "    rnn.cuda(0)\n",
    "\n",
    "#Defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "#Saving the trained model\n",
    "util.save_model('trained_rnn_5', trained_rnn)\n",
    "print('Model Trained and Saved')\n",
    "np.save('loss-model-5.npy', np.array(losses))\n",
    "print(\"Time taken: \", (time.time() - now)/60, \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
